{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP8cQufoXV4j+7QI2mc+2Xp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/git-grace/experiment/blob/main/base/preprocess_ner.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Imlgzey3ZHEK"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = \"i want watch movies on visha\"\n",
        "# word-type-start-end\n",
        "entities = [(\"visha\", \"App\", 23, 28)]"
      ],
      "metadata": {
        "id": "8HEC3WMrZSpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence[23:28]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "uQHy1UG_Nt4e",
        "outputId": "1e76cf67-9cb0-4371-c7a4-c6812a62a6e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'visha'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n"
      ],
      "metadata": {
        "id": "2bhP-6A3ZkA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = tokenizer(sentence, return_offsets_mapping=True)\n",
        "tokens"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T8DgnxrsaTuo",
        "outputId": "451d1282-c06f-4a86-dc7a-5d7c153e09ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 1045, 2215, 3422, 5691, 2006, 25292, 3270, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1], 'offset_mapping': [(0, 0), (0, 1), (2, 6), (7, 12), (13, 19), (20, 22), (23, 26), (26, 28), (0, 0)]}"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words = tokenizer.convert_ids_to_tokens(tokens[\"input_ids\"])\n",
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwNEUS5tas4a",
        "outputId": "08f1c2d4-8585-4f0d-ce36-ffe5697b3148"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['[CLS]', 'i', 'want', 'watch', 'movies', 'on', 'vis', '##ha', '[SEP]']"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def build_entities_by_token(entity_list, token_offsets):\n",
        "  entities_by_token = []\n",
        "  edge_mismatch = False\n",
        "  start2id = {}\n",
        "  end2id = {}\n",
        "  for i, (s, e) in enumerate(token_offsets):\n",
        "    if e == 0:\n",
        "      continue\n",
        "    start2id[s] = i\n",
        "    end2id[e] = i+1\n",
        "\n",
        "  print(\"start2id: \", start2id, \"end2id: \", end2id)\n",
        "  for w, t, s, e in entity_list:\n",
        "    sid = start2id.get(s, 0)\n",
        "    eid = end2id.get(e, 0)\n",
        "    if sid and eid and (eid > sid):\n",
        "      entities_by_token.append((w, t, sid, eid))\n",
        "    else:\n",
        "      edge_mismatch = True\n",
        "    print(w, t, sid, eid)\n",
        "  return entities_by_token, edge_mismatch\n",
        "\n",
        "\n",
        "class TrainingData:\n",
        "  def __init__(self, sub_words, tokens, entities):\n",
        "    self.sub_words = sub_words\n",
        "    self.input_ids = tokens[\"input_ids\"]\n",
        "    self.length = len(tokens[\"input_ids\"])\n",
        "    self.token_type_ids = tokens[\"token_type_ids\"]\n",
        "    self.attention_mask = tokens[\"attention_mask\"]\n",
        "    self.offset_mapping = tokens[\"offset_mapping\"]\n",
        "    self.entities_tokens, self.edge_match = build_entities_by_token(entities, tokens[\"offset_mapping\"])\n",
        "\n",
        "  def get_tags(self, max_len):\n",
        "    tags = [\"O\" for _ in range(max_len)]\n",
        "    tags[0] = \"START_TAG\"\n",
        "    tags[self.length-1] = \"END_TAG\"\n",
        "    for w, t, s, e in self.entities_tokens:\n",
        "      tags[s] = f'B-{t}'\n",
        "      for j in range(s+1, e):\n",
        "        tags[j] = f\"I-{t}\"\n",
        "    return tags\n",
        "\n",
        "\n",
        "train_data = TrainingData(\n",
        "  sub_words=words,\n",
        "  tokens=tokens,\n",
        "  entities=entities,\n",
        ")\n",
        "\n",
        "tags = train_data.get_tags(max_len=10)\n",
        "for w, t in zip(words, tags):\n",
        "  print(w, t)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-YP09OIauLz",
        "outputId": "78ff5355-c3a4-4f29-bb95-5e2419a572a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start2id:  {0: 1, 2: 2, 7: 3, 13: 4, 20: 5, 23: 6, 26: 7} end2id:  {1: 2, 6: 3, 12: 4, 19: 5, 22: 6, 26: 7, 28: 8}\n",
            "visha App 6 8\n",
            "[CLS] START_TAG\n",
            "i O\n",
            "want O\n",
            "watch O\n",
            "movies O\n",
            "on O\n",
            "vis B-App\n",
            "##ha I-App\n",
            "[SEP] END_TAG\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install pytorch-crf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VrZRLImYQ0cl",
        "outputId": "44e3910a-86ed-442e-85da-4b544dfd404a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch-crf\n",
            "  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n",
            "Installing collected packages: pytorch-crf\n",
            "Successfully installed pytorch-crf-0.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchcrf import CRF\n",
        "from itertools import repeat\n",
        "from transformers import BertModel"
      ],
      "metadata": {
        "id": "EqYN1TgaeGD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CRFModel(nn.Module):\n",
        "  def __init__(self, bert_dir, num_tags, dropout_prob=0.1, **kwargs):\n",
        "    super(CRFModel, self).__init__()\n",
        "\n",
        "    self.bert_module = BertModel.from_pretrained(bert_dir)\n",
        "\n",
        "    out_dims = kwargs.pop(\"hidden_size\", 768)\n",
        "    mid_linear_dims = kwargs.pop('mid_linear_dims', 128)\n",
        "\n",
        "    self.mid_linear = nn.Sequential(\n",
        "      nn.Linear(out_dims, mid_linear_dims),\n",
        "      nn.ReLU(),\n",
        "      nn.Dropout(dropout_prob)\n",
        "    )\n",
        "\n",
        "    out_dims = mid_linear_dims\n",
        "    self.classifier = nn.Linear(out_dims, num_tags)\n",
        "\n",
        "    self.loss_weight = nn.Parameter(torch.FloatTensor(1), requires_grad=True)\n",
        "    self.loss_weight.data.fill_(-0.2)\n",
        "\n",
        "    self.crf_module = CRF(num_tags=num_tags, batch_first=True)\n",
        "\n",
        "    init_blocks = [self.mid_linear, self.classifier]\n",
        "\n",
        "    self._init_weights(init_blocks, initializer_range=self.bert_config.initializer_range)\n",
        "\n",
        "  def forward(self,\n",
        "            token_ids,\n",
        "            attention_masks,\n",
        "            token_type_ids,\n",
        "            labels=None,\n",
        "            pseudo=None):\n",
        "\n",
        "    bert_outputs = self.bert_module(input_ids=token_ids, attention_mask=attention_masks, token_type_ids=token_type_ids)\n",
        "\n",
        "    # 常规\n",
        "    seq_out = bert_outputs[0]\n",
        "\n",
        "    seq_out = self.mid_linear(seq_out)\n",
        "\n",
        "    emissions = self.classifier(seq_out)\n",
        "\n",
        "    if labels is not None:\n",
        "      tokens_loss = -1. * self.crf_module(emissions=emissions,\n",
        "                          tags=labels.long(),\n",
        "                          mask=attention_masks.byte(),\n",
        "                          reduction='mean')\n",
        "\n",
        "      out = (tokens_loss,)\n",
        "\n",
        "    else:\n",
        "      tokens_out = self.crf_module.decode(emissions=emissions, mask=attention_masks.byte())\n",
        "\n",
        "      out = (tokens_out, emissions)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "Bxi6EKVZQzxI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class InputExample:\n",
        "  def __init__(self,\n",
        "        set_type,\n",
        "        text,\n",
        "        labels=None):\n",
        "    self.set_type = set_type\n",
        "    self.text = text\n",
        "    self.labels = labels\n",
        "\n",
        "def convert_crf_example(ex_idx, example: InputExample, tokenizer, max_seq_len, ent2id):\n",
        "  set_type = example.set_type\n",
        "  raw_text = example.text\n",
        "  entities = example.labels\n",
        "  pseudo = example.pseudo\n",
        "\n",
        "  callback_info = (raw_text,)\n",
        "\n",
        "  tokens = fine_grade_tokenize(raw_text, tokenizer)\n",
        "  assert len(tokens) == len(raw_text)\n",
        "\n",
        "  label_ids = None\n",
        "\n",
        "  if set_type == 'train':\n",
        "    # information for dev callback\n",
        "    label_ids = [0] * len(tokens)\n",
        "\n",
        "    # tag labels  ent ex. (T1, DRUG_DOSAGE, 447, 450, 小蜜丸)\n",
        "    for ent in entities:\n",
        "        ent_type = ent[0]\n",
        "\n",
        "        ent_start = ent[-1]\n",
        "        ent_end = ent_start + len(ent[1]) - 1\n",
        "\n",
        "        if ent_start == ent_end:\n",
        "            label_ids[ent_start] = ent2id['S-' + ent_type]\n",
        "        else:\n",
        "            label_ids[ent_start] = ent2id['B-' + ent_type]\n",
        "            label_ids[ent_end] = ent2id['E-' + ent_type]\n",
        "            for i in range(ent_start + 1, ent_end):\n",
        "                label_ids[i] = ent2id['I-' + ent_type]\n",
        "\n",
        "    if len(label_ids) > max_seq_len - 2:\n",
        "        label_ids = label_ids[:max_seq_len - 2]\n",
        "\n",
        "    label_ids = [0] + label_ids + [0]\n",
        "\n",
        "    # pad\n",
        "    if len(label_ids) < max_seq_len:\n",
        "        pad_length = max_seq_len - len(label_ids)\n",
        "        label_ids = label_ids + [0] * pad_length  # CLS SEP PAD label都为O\n",
        "\n",
        "    assert len(label_ids) == max_seq_len, f'{len(label_ids)}'\n",
        "\n",
        "  encode_dict = tokenizer.encode_plus(text=tokens,\n",
        "                                      max_length=max_seq_len,\n",
        "                                      pad_to_max_length=True,\n",
        "                                      is_pretokenized=True,\n",
        "                                      return_token_type_ids=True,\n",
        "                                      return_attention_mask=True)\n",
        "\n",
        "  token_ids = encode_dict['input_ids']\n",
        "  attention_masks = encode_dict['attention_mask']\n",
        "  token_type_ids = encode_dict['token_type_ids']\n",
        "\n",
        "  # if ex_idx < 3:\n",
        "  #     logger.info(f\"*** {set_type}_example-{ex_idx} ***\")\n",
        "  #     logger.info(f'text: {\" \".join(tokens)}')\n",
        "  #     logger.info(f\"token_ids: {token_ids}\")\n",
        "  #     logger.info(f\"attention_masks: {attention_masks}\")\n",
        "  #     logger.info(f\"token_type_ids: {token_type_ids}\")\n",
        "  #     logger.info(f\"labels: {label_ids}\")\n",
        "\n",
        "  feature = CRFFeature(\n",
        "      # bert inputs\n",
        "      token_ids=token_ids,\n",
        "      attention_masks=attention_masks,\n",
        "      token_type_ids=token_type_ids,\n",
        "      labels=label_ids,\n",
        "      pseudo=pseudo\n",
        "  )\n",
        "\n",
        "  return feature, callback_info"
      ],
      "metadata": {
        "id": "J_pn-OKJQxPS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}